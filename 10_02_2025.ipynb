{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPv0Yoc7oflxCUswH2c6CDE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramakrishna12343/NLP-Assignment/blob/main/10_02_2025.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GEjVTku9Uo8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHcxP-ETUolK",
        "outputId": "f6ca1a0c-636f-469a-d3c5-c61898ef21cc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2\n",
        "from PyPDF2 import PdfFileReader"
      ],
      "metadata": {
        "id": "k6Ww70YgVDZh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKW6PsEJTCOO",
        "outputId": "769c9051-d200-4d4f-f0cd-b2db131683d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of pages:  35\n",
            " \n",
            " \n",
            " Development  Plan for Greater Mumbai 2014‐2034                                                                                                                                                                                                                                                      \n",
            "Acknowledgements  \n",
            "The Consultant  wishes to thank the following  individuals  from the Municipal  Corporation  of \n",
            "Greater Mumbai for their invaluable  support, insights and contributions  towards ‘Working  Paper 1 \n",
            "– Preparation  of Base Map’ for the preparation  of the Development  Plan for Greater Mumbai \n",
            "2014‐34. \n",
            " Mr. Subodh Kumar, IAS, Municipal  Commissioner;  \n",
            " Mr. Rajeev Kuknoor, Chief Engineer Development  Plan; \n",
            " Mr. Sudhir Ghate, Deputy Chief Engineer Development  Plan; \n",
            " Mr. A.G. Marathe, Deputy Chief Engineer Development  Plan; \n",
            " Mr. R. Balachandran,  Executive  Engineer and Town Planning Officer, Development  Plan. \n",
            " Our gratitude  to the following  experts for their invaluable  insights and support: \n",
            " \n",
            "Mr. V.K Phatak, Former Chief Town Planner (MMRDA);  \n",
            " Mr. A.N Kale, Former Chief Engineer, (DP); \n",
            " Mr. A. S Jain Former Dy. Chief Engineer, (DP). \n",
            " We wish to especially  thank MCGM officers, Mr. Jagdish Talreja, Mr. Dinesh Naik, Mr. Hiren \n",
            "Daftardar,  Ms. Anita Naik for their continual  support since the\n",
            " beginning  of the project and their \n",
            "help towards familiarization  and data collection.  They have been instrumental  in helping to \n",
            "contact various MCGM departments  as well as in helping to establish contact with personnel  from \n",
            "other government  departments  and organizations.  Many thanks for the MCGM team, for \n",
            "deploying  personnel,  particularly  Mr. Prasad Gharat, on extensive  field visits that have helped in \n",
            "understanding  actual ground conditions.  \n",
            " \n",
            "We apologize  if we have inadvertently  omitted anyone to whom acknowledgement  is due. We hope \n",
            "and anticipate  the work's usefulness  for the intended purpose. \n",
            " \n"
          ]
        }
      ],
      "source": [
        "pdf=open('file1pdf.pdf','rb')\n",
        "pdf_reader=PyPDF2.PdfReader(pdf)\n",
        "print('Number of pages: ',len(pdf_reader.pages))\n",
        "page=pdf_reader.pages[1]\n",
        "print(page.extract_text())\n",
        "pdf.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2, urllib, nltk\n",
        "from io import BytesIO\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "n7NMADCLUiTo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2Fwq0SLUnz1",
        "outputId": "f80b6684-5ff8-42af-a528-71ba589a303b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile=urllib.request.urlopen('http://www.udri.org/pdf/02%20working%20paper%201.pdf')\n",
        "pdf=PyPDF2.PdfReader(BytesIO(wFile.read()))"
      ],
      "metadata": {
        "id": "14JVdA1yVdz7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6DhcFVvVj34",
        "outputId": "d5e98de2-b4d6-4b44-ef54-6e13d2a2cefe"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pageObj = pdf.pages[4]\n",
        "\n",
        "page2 = pageObj.extract_text()\n",
        "\n",
        "#Cleaning the text\n",
        "\n",
        "punctuations = ['(',')',';',':','[',']',',','...','.']\n",
        "\n",
        "tokens = word_tokenize(page2)\n",
        "\n",
        "stop_words = stopwords.words('english')\n",
        "\n",
        "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
      ],
      "metadata": {
        "id": "sIZOgCSiW8B0"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "keywords"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr4clcqWXrj5",
        "outputId": "e6b2322d-a7fb-4c68-9b6f-74299a2661f8"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PLU',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Proposed',\n",
              " 'Land',\n",
              " 'use',\n",
              " '..........................................................................................................................',\n",
              " '5',\n",
              " 'SRA',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Slum',\n",
              " 'Rehabilitation',\n",
              " 'Authority',\n",
              " '.........................................................................................................',\n",
              " '5',\n",
              " 'SWOT',\n",
              " '...............................................................................................................................',\n",
              " '...............',\n",
              " '5',\n",
              " 'Strengths',\n",
              " 'Weaknesses',\n",
              " 'Opportunities',\n",
              " 'Threats',\n",
              " '..................................................................................',\n",
              " '5',\n",
              " 'TDR',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Transfer',\n",
              " 'Development',\n",
              " 'Rights',\n",
              " '......................................................................................................',\n",
              " '5',\n",
              " 'ToR',\n",
              " '...............................................................................................................................',\n",
              " '..................',\n",
              " '5',\n",
              " 'Terms',\n",
              " 'Reference',\n",
              " '.........................................................................................................................',\n",
              " '5',\n",
              " '1.Introduction',\n",
              " '...............................................................................................................................',\n",
              " '..',\n",
              " '6',\n",
              " '2.Data',\n",
              " 'made',\n",
              " 'available',\n",
              " 'Preparation',\n",
              " 'Base',\n",
              " 'Map',\n",
              " 'MCGM',\n",
              " '.......................................................',\n",
              " '6',\n",
              " 'Quick',\n",
              " 'Bird',\n",
              " 'satellite',\n",
              " 'image',\n",
              " 'handed',\n",
              " 'MCGM',\n",
              " '.........................................................................',\n",
              " '6',\n",
              " 'b',\n",
              " 'Digitized',\n",
              " 'Tikka',\n",
              " 'Sheets',\n",
              " 'Shape',\n",
              " 'Files',\n",
              " '.................................................................................................',\n",
              " '7',\n",
              " 'c',\n",
              " 'Other',\n",
              " 'Data',\n",
              " 'Layers',\n",
              " 'forming',\n",
              " 'present',\n",
              " 'Development',\n",
              " 'Plan',\n",
              " 'Shape',\n",
              " 'Files',\n",
              " '..............................................',\n",
              " '7',\n",
              " '3.Components',\n",
              " 'Base',\n",
              " 'Map',\n",
              " '...........................................................................................................',\n",
              " '8',\n",
              " 'Base',\n",
              " 'layer',\n",
              " '...............................................................................................................................',\n",
              " '.........',\n",
              " '8',\n",
              " 'b',\n",
              " 'Data',\n",
              " 'layers',\n",
              " '...............................................................................................................................',\n",
              " '........',\n",
              " '9',\n",
              " 'c',\n",
              " 'Data',\n",
              " 'updated',\n",
              " '..........................................................................................................................',\n",
              " '9',\n",
              " '4.Quality',\n",
              " 'Data',\n",
              " 'Received',\n",
              " '............................................................................................................',\n",
              " '10',\n",
              " 'Background',\n",
              " '...............................................................................................................................',\n",
              " '....',\n",
              " '10',\n",
              " 'b',\n",
              " 'GIS',\n",
              " 'Issues',\n",
              " 'resolution',\n",
              " '....................................................................................................',\n",
              " '10',\n",
              " '...............................................................................................................................',\n",
              " '...............',\n",
              " '11',\n",
              " 'c',\n",
              " 'Summary',\n",
              " 'Assessment',\n",
              " 'Matrix',\n",
              " 'GIS',\n",
              " 'Data',\n",
              " 'wards',\n",
              " '................................................',\n",
              " '17',\n",
              " '5.Way',\n",
              " 'Forward',\n",
              " '..............................................................................................................................',\n",
              " '19']"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "name_list = list()\n",
        "\n",
        "check =  ['Mr.', 'Mrs.', 'Ms.']\n",
        "\n",
        "for idx, token in enumerate(tokens):\n",
        "\n",
        "    if token.startswith(tuple(check)) and idx < (len(tokens)-1):\n",
        "\n",
        "        name = token + tokens[idx+1] + ' ' +  tokens[idx+2]\n",
        "\n",
        "        name_list.append(name)\n",
        "print(name_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKZHq3zPX2gL",
        "outputId": "b373a280-e198-459a-c7fc-b9adcfb77168"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wFile.close()"
      ],
      "metadata": {
        "id": "7E2DkGrJYh8H"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ke19ObKtaCD8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdz2fhawZkm2",
        "outputId": "58f1ecda-7c27-465f-a141-6dd1843705f9"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from python-docx) (4.12.2)\n",
            "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/244.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m235.5/244.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import docx"
      ],
      "metadata": {
        "id": "P3wpTtNEZzUZ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = open(\"AT-15_Research paper.docx\",\"rb\")\n",
        "\n",
        "#creating word reader object\n",
        "\n",
        "document = docx.Document(doc)\n",
        "\n"
      ],
      "metadata": {
        "id": "z-Wp_p7bZ5W3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docu=\"\"\n",
        "\n",
        "for para in document.paragraphs:\n",
        "\n",
        "    docu += para.text\n",
        "\n",
        "#to see the output call docu\n",
        "\n",
        "print(docu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB050ooGaWg3",
        "outputId": "0aa9382f-3bc3-41ba-8429-c224f29c8c17"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAFFIC SIGN RECONITION USINGDEEP LEARNING1 Dr.A.Sivaranjani2 B.Harsha Vardhan,3 B.Chinna Madhu,4B.Pradeep5 B.Poojith Goud,6Ch.Rama KrishnaArtificial Intelligence & MachineLearningDepartment Of Computer Science And EngineeringMalla Reddy University, Hyderabad,Telangana,India             ABSTRACT                               Traffic sign recognition is a critical component of intelligent transportation systems, enhancing road safety and enabling autonomous vehicle navigation. This paper presents a novel approach to traffic sign recognition using deep learning techniques, specifically convolutional neural networks (CNNs).We developed a robust model trained on a diverse dataset of traffic signs, encompassing various shapes, colors, and conditions to ensure high accuracy and reliability. The proposed system incorporates data augmentation and transfer learning to improve performance in real-world scenarios. Additionally, we discuss the challenges faced, such as variations in lighting and occlusions, and propose solutions to address these issues. Our findings underscore the potential of deep learning in enhancing traffic sign recognition systems, paving the way for safer and more efficient roadways.I.INTRODUCTIONTraffic Sign Recognition (TSR) is a critical technology in modern transportation systems, playing a key role in enhancing road safety and enabling autonomous driving. It allows vehicles and intelligent transportation systems to identify and interpret traffic signs, ensuring compliance with road rules and improving navigation efficiency. TSR is essential in reducing accidents, providing timely alerts to drivers, and enabling fully autonomous vehicles to make informed decisions in real time.Traditional TSR systems relied heavily on handcrafted features such as color, shape, and texture, combined with classical machine learning algorithms. While effective in controlled environments, these approaches often struggled in real-world scenarios involving varying weather conditions, poor lighting, occlusions, and distorted views of traffic signs.The advent of deep learning has revolutionized TSR by enabling models to automatically learn and extract features from raw image data. Convolutional Neural Networks (CNNs), in particular, have proven to be highly effective for image-based tasks, including traffic sign detection and classification. Deep learning models have demonstrated superior performance in handling the complexities of real-world environments, achieving remarkable accuracy and robustness.Modern TSR systems typically involve two main stages: detection (locating traffic signs in images or video frames) and classification (identifying the specific type of traffic sign). Advanced neural network architectures such as Faster R-CNN, YOLO, and SSD integrate these tasks into a single framework, enabling efficient real-time recognition. Furthermore, emerging techniques like Vision Transformers and attention mechanisms are pushing the boundaries of TSR capabilities, making the systems more accurate and versatile.Despite these advancements, challenges such as class imbalance, computational complexity, and adaptability to diverse traffic sign standards across regions remain. Continuous research is focused on addressing these issues through improved datasets, data augmentation, lightweight models for real-time applications, and transfer learning.The applications of deep learning-based TSR are vast, including autonomous driving systems, Advanced Driver Assistance Systems (ADAS), and traffic monitoring in smart cities. As the field evolves, deep learning continues to drive innovations in TSR, paving the way for safer and more efficient transportation systems.II. LITERATURE REVIEWTraffic sign recognition (TSR) is a crucial component of intelligent transportation systems and autonomous vehicles, enabling them to interpret and respond to traffic signs accurately. TSR involves two primary tasks: detecting traffic signs in images and classifying them into predefined categories. Traditional methods relied on handcrafted features such as color, shape, and texture, paired with classical machine learning algorithms. However, these approaches often struggled with real-world challenges like variable lighting, occlusions, and distorted perspectives.Deep learning has transformed TSR by leveraging neural network architectures, particularly Convolutional Neural Networks (CNNs), to automatically extract and learn features directly from image data. Popular datasets like the German Traffic Sign Recognition Benchmark (GTSRB), Belgian Traffic Sign Dataset (BTSD), and LISA Traffic Sign Dataset have played a pivotal role in developing and benchmarking deep learning models for TSR. These datasets, alongside synthetic data generation techniques, have enabled the creation of robust systems capable of handling diverse conditions.Advanced deep learning models such as Faster R-CNN, YOLO, and SSD integrate detection and classification, offering high accuracy and real-time performance. Emerging techniques like Vision Transformers (ViT) and attention mechanisms further enhance the ability to recognize traffic signs in complex scenarios. Despite these advancements, challenges like real-world variability, class imbalance, and computational demands remain significant. Researchers are addressing these through data augmentation, transfer learning, lightweight architectures, and hybrid models combining CNNs with recurrent layers for video-based applications.The applications of deep learning-based TSR are vast, ranging from autonomous driving systems and Advanced Driver Assistance Systems (ADAS) to traffic monitoring in smart cities. Future research aims to develop more diverse datasets, optimize models for edge devices, and improve generalization across regions and conditions. Deep learning continues to advance TSR, driving safer and smarter transportation solutions.III.PROBLEM STATEMENTTraffic signs are essential for maintaining road safety and providing critical information to drivers and autonomous systems. However, recognizing traffic signs accurately and efficiently in real-world environments poses significant challenges due to varying lighting conditions, weather effects, occlusions, faded or damaged signs, and differences in sign designs across regions. Traditional methods relying on handcrafted features and classical machine learning struggle to handle these complexities effectively.The problem lies in developing a robust, efficient, and real-time Traffic Sign Recognition (TSR) system that can accurately detect and classify traffic signs under diverse environmental conditions. Such a system must be capable of handling a wide variety of traffic sign categories while addressing issues like class imbalance and computational limitations for deployment in edge devices or autonomous vehicles. Furthermore, the system should generalize well to new regions with different traffic sign standards and maintain reliability in dynamic scenarios like moving vehicles or urban environments.The goal is to leverage deep learning techniques, particularly Convolutional Neural Networks (CNNs) and advanced architectures like YOLO, Faster R-CNN, and Vision Transformers, to create a TSR system that achieves high accuracy, robustness, and real-time performance, ensuring safer and smarter transportation systems.IV. METHODOLOGYThe methodology for Traffic Sign Recognition (TSR) using deep learning encompasses a series of critical steps that ensure the system can accurately detect and classify traffic signs under varying real-world conditions. The goal is to create a robust and efficient system that can handle challenges such as different lighting conditions, environmental variations, and occlusions.1. Data Collection and PreprocessingTraffic sign recognition begins with gathering relevant datasets such as the German Traffic Sign Recognition Benchmark (GTSRB), Belgian Traffic Sign Dataset (BTSD), or LISA Traffic Sign Dataset. These datasets are annotated with bounding boxes for detection and labels for classification. Data preprocessing involves resizing images to a uniform size, normalizing pixel values, and applying augmentation techniques like rotations, scaling, and flipping. This increases the diversity of training data and improves model robustness under real-world conditions.2. Model DesignUse deep learning architectures such as Faster R-CNN, YOLO, or SSD for detection, and CNNs like ResNet or MobileNet for classification. Incorporate attention mechanisms for improved focus and lightweight models for edge device deployment.3. Training the ModelTraining involves splitting the dataset into training, validation, and testing sets. Loss functions such as cross-entropy for classification and multi-task loss for detection are used. Optimizers like Adam or SGD adjust model weights during training. Regularization techniques such as dropout and early stopping are employed to prevent overfitting, while hyperparameter tuning helps optimize model performance.4. Model EvaluationThe performance of the trained model is assessed using key metrics to ensure it meets desired accuracy levels. For detection tasks, Intersection over Union (IoU), precision, recall, and mean Average Precision (mAP) are calculated to measure how well the model locates and identifies traffic signs. For classification tasks, metrics such as accuracy, precision, recall, and F1-score are used to evaluate how well the model assigns the correct label to each sign. Testing on a diverse, unseen dataset is crucial to verify the model's ability to generalize and avoid overfitting.5. DeploymentOnce trained, the model is optimized for deployment in real-time systems such as autonomous vehicles or Advanced Driver Assistance Systems (ADAS). Techniques like pruning, quantization, or knowledge distillation are applied to reduce the model's size and computational demands, ensuring it can run efficiently on edge devices with limited resources. Tools like TensorRT, OpenVINO, or custom hardware accelerators are used to accelerate inference and ensure the model can process traffic signs in real-time, crucial for safety in autonomous driving systems.6.DeploymentMonitoringand UpdatesAfter deployment, continuous monitoring of the model’s performance in real-world conditions is essential to identify any potential issues, such as misclassifications or failures in challenging environments. Data collected from real-time use can be leveraged to fine-tune the model and retrain it periodically to improve its accuracy and adaptability. This ongoing learning helps the system stay up-to-date with new traffic sign designs, environmental changes, or regional variations in traffic regulations, enhancing the robustness and long-term reliability of the recognition system.V. ARCHITECTUREER Diagram VI.EXECUTIONFig1.1 Image detectedVII.CONCLUSIONTraffic Sign Recognition (TSR) using deep learning has become a vital technology for autonomous driving and advanced driver-assistance systems (ADAS). By using deep learning models like convolutional neural networks (CNNs), TSR systems can accurately detect and classify traffic signs, enhancing road safety and navigation.While significant progress has been made, challenges remain, including handling regional sign variations, ensuring robustness in diverse conditions, and optimizing models for real-time deployment on resource-constrained devices. Future research should focus on improving model generalization, enabling continuous learning, and increasing system efficiency.With ongoing advancements, TSR systems will become more reliable and integral to the development of autonomous vehicles, contributing to safer and more efficient transportation.VIII. FUTURE WORKHandling Variability in Traffic Signs\n",
            "Traffic signs vary in shape, color, and design across different countries and regions. Future research could focus on developing models that are adaptable to these variations, improving generalization to diverse sign designs. Techniques like domain adaptation could help fine-tune models for specific regions or sign types.Improving Robustness in Adverse Conditions\n",
            "TSR systems must perform reliably under various environmental conditions such as weather (rain, fog, snow), lighting (dawn, dusk, night), or partial occlusion of signs. Future work could explore advanced data augmentation, multi-spectral image analysis (using both RGB and infrared images), and models that improve robustness in challenging conditions.Real-Time and Low-Resource Deployment\n",
            "Deep learning-based TSR models often require substantial computational resources. Future work could focus on optimizing models for real-time deployment on edge devices by using model compression techniques like pruning, quantization, and knowledge distillation, enabling efficient performance on resource-constrained platforms.Continuous Learning and Model Adaptation\n",
            "TSR systems may encounter new or evolving traffic signs and changing environmental conditions. Implementing continuous learning techniques, such as online learning or active learning, would allow models to adapt over time, improving performance without needing full retraining.Explainability and Transparency\n",
            "As deep learning models become more complex, enhancing transparency becomes essential. Future research could focus on explainable AI (XAI) methods for TSR, helping developers and end-users understand model decisions, fostering trust, and ensuring safety in real-world deployments.IX. REFERENCES1.Dollár, P., Appel, R., Belongie, S., & Perona, P. (2012). Fast Feature Pyramids for Object Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(8), 1532-1545.\n",
            "This paper discusses techniques for improving object detection, including methods that are applicable for detecting traffic signs in images.2.Yolo, J. (2016). You Only Look Once: Unified, Real-Time Object Detection. arXiv:1506.02640.\n",
            "The original paper on the YOLO (You Only Look Once) object detection framework, which is commonly used for detecting traffic signs in real-time systems.3.Berg, D., & Puch, M. (2018). Traffic Sign Recognition with Deep Convolutional Networks. In Proceedings of the International Conference on Computer Vision (ICCV).\n",
            "This paper investigates deep learning techniques for recognizing traffic signs and discusses the use of convolutional neural networks (CNNs) for classification.4.Grigorescu, S., Radoslav, L., & Visan, A. (2018). Deep Learning for Traffic Sign Recognition. Machine Vision and Applications, 29(1), 79-94.\n",
            "This work provides an in-depth look at applying deep learning techniques specifically for traffic sign recognition, exploring various CNN architectures and evaluation methods.5.Awais, M., & Asghar, S. (2020). Traffic Sign Recognition Using Convolutional Neural Networks and Region-based CNN. In Proceedings of the 5th International Conference on Artificial Intelligence and Computer Vision (AICV).\n",
            "This paper discusses the implementation of region-based CNNs, such as Faster R-CNN, for traffic sign detection and classification.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for i in range(len(document.paragraphs)):\n",
        "\n",
        "    print(\"The content of the paragraph \"+ str(i)+\" is ：\" + document.paragraphs[i].text+\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhWTSs5Wbr-J",
        "outputId": "f24f1659-fe0f-4d84-8571-529309a218bc"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The content of the paragraph 0 is ：\n",
            "\n",
            "The content of the paragraph 1 is ：TRAFFIC SIGN RECONITION USING\n",
            "\n",
            "The content of the paragraph 2 is ：DEEP LEARNING\n",
            "\n",
            "The content of the paragraph 3 is ：1 Dr.A.Sivaranjani\n",
            "\n",
            "The content of the paragraph 4 is ：2 B.Harsha Vardhan,3 B.Chinna Madhu,4B.Pradeep\n",
            "\n",
            "The content of the paragraph 5 is ：5 B.Poojith Goud,6Ch.Rama Krishna\n",
            "\n",
            "The content of the paragraph 6 is ：Artificial Intelligence & MachineLearning\n",
            "\n",
            "The content of the paragraph 7 is ：Department Of Computer Science And Engineering\n",
            "\n",
            "The content of the paragraph 8 is ：Malla Reddy University, Hyderabad,Telangana,India\n",
            "\n",
            "The content of the paragraph 9 is ：             ABSTRACT                               \n",
            "\n",
            "The content of the paragraph 10 is ：Traffic sign recognition is a critical component of intelligent transportation systems, enhancing road safety and enabling autonomous vehicle navigation. This paper presents a novel approach to traffic sign recognition using deep learning techniques, specifically convolutional neural networks (CNNs).\n",
            "\n",
            "The content of the paragraph 11 is ：We developed a robust model trained on a diverse dataset of traffic signs, encompassing various shapes, colors, and conditions to ensure high accuracy and reliability. The proposed system incorporates data augmentation and transfer learning to improve performance in real-world scenarios. \n",
            "\n",
            "The content of the paragraph 12 is ：Additionally, we discuss the challenges faced, such as variations in lighting and occlusions, and propose solutions to address these issues. Our findings underscore the potential of deep learning in enhancing traffic sign recognition systems, paving the way for safer and more efficient roadways.\n",
            "\n",
            "The content of the paragraph 13 is ：\n",
            "\n",
            "The content of the paragraph 14 is ：I.INTRODUCTION\n",
            "\n",
            "The content of the paragraph 15 is ：Traffic Sign Recognition (TSR) is a critical technology in modern transportation systems, playing a key role in enhancing road safety and enabling autonomous driving. It allows vehicles and intelligent transportation systems to identify and interpret traffic signs, ensuring compliance with road rules and improving navigation efficiency. TSR is essential in reducing accidents, providing timely alerts to drivers, and enabling fully autonomous vehicles to make informed decisions in real time.\n",
            "\n",
            "The content of the paragraph 16 is ：Traditional TSR systems relied heavily on handcrafted features such as color, shape, and texture, combined with classical machine learning algorithms. While effective in controlled environments, these approaches often struggled in real-world scenarios involving varying weather conditions, poor lighting, occlusions, and distorted views of traffic signs.\n",
            "\n",
            "The content of the paragraph 17 is ：The advent of deep learning has revolutionized TSR by enabling models to automatically learn and extract features from raw image data. Convolutional Neural Networks (CNNs), in particular, have proven to be highly effective for image-based tasks, including traffic sign detection and classification. Deep learning models have demonstrated superior performance in handling the complexities of real-world environments, achieving remarkable accuracy and robustness.\n",
            "\n",
            "The content of the paragraph 18 is ：Modern TSR systems typically involve two main stages: detection (locating traffic signs in images or video frames) and classification (identifying the specific type of traffic sign). Advanced neural network architectures such as Faster R-CNN, YOLO, and SSD integrate these tasks into a single framework, enabling efficient real-time recognition. Furthermore, emerging techniques like Vision Transformers and attention mechanisms are pushing the boundaries of TSR capabilities, making the systems more accurate and versatile.\n",
            "\n",
            "The content of the paragraph 19 is ：Despite these advancements, challenges such as class imbalance, computational complexity, and adaptability to diverse traffic sign standards across regions remain. Continuous research is focused on addressing these issues through improved datasets, data augmentation, lightweight models for real-time applications, and transfer learning.\n",
            "\n",
            "The content of the paragraph 20 is ：The applications of deep learning-based TSR are vast, including autonomous driving systems, Advanced Driver Assistance Systems (ADAS), and traffic monitoring in smart cities. As the field evolves, deep learning continues to drive innovations in TSR, paving the way for safer and more efficient transportation systems.\n",
            "\n",
            "The content of the paragraph 21 is ：\n",
            "\n",
            "The content of the paragraph 22 is ：II. LITERATURE REVIEW\n",
            "\n",
            "The content of the paragraph 23 is ：Traffic sign recognition (TSR) is a crucial component of intelligent transportation systems and autonomous vehicles, enabling them to interpret and respond to traffic signs accurately. TSR involves two primary tasks: detecting traffic signs in images and classifying them into predefined categories. Traditional methods relied on handcrafted features such as color, shape, and texture, paired with classical machine learning algorithms. However, these approaches often struggled with real-world challenges like variable lighting, occlusions, and distorted perspectives.\n",
            "\n",
            "The content of the paragraph 24 is ：Deep learning has transformed TSR by leveraging neural network architectures, particularly Convolutional Neural Networks (CNNs), to automatically extract and learn features directly from image data. Popular datasets like the German Traffic Sign Recognition Benchmark (GTSRB), Belgian Traffic Sign Dataset (BTSD), and LISA Traffic Sign Dataset have played a pivotal role in developing and benchmarking deep learning models for TSR. These datasets, alongside synthetic data generation techniques, have enabled the creation of robust systems capable of handling diverse conditions.\n",
            "\n",
            "The content of the paragraph 25 is ：Advanced deep learning models such as Faster R-CNN, YOLO, and SSD integrate detection and classification, offering high accuracy and real-time performance. Emerging techniques like Vision Transformers (ViT) and attention mechanisms further enhance the ability to recognize traffic signs in complex scenarios. Despite these advancements, challenges like real-world variability, class imbalance, and computational demands remain significant. Researchers are addressing these through data augmentation, transfer learning, lightweight architectures, and hybrid models combining CNNs with recurrent layers for video-based applications.\n",
            "\n",
            "The content of the paragraph 26 is ：The applications of deep learning-based TSR are vast, ranging from autonomous driving systems and Advanced Driver Assistance Systems (ADAS) to traffic monitoring in smart cities. Future research aims to develop more diverse datasets, optimize models for edge devices, and improve generalization across regions and conditions. Deep learning continues to advance TSR, driving safer and smarter transportation solutions.\n",
            "\n",
            "The content of the paragraph 27 is ：\n",
            "\n",
            "The content of the paragraph 28 is ：III.PROBLEM STATEMENT\n",
            "\n",
            "The content of the paragraph 29 is ：Traffic signs are essential for maintaining road safety and providing critical information to drivers and autonomous systems. However, recognizing traffic signs accurately and efficiently in real-world environments poses significant challenges due to varying lighting conditions, weather effects, occlusions, faded or damaged signs, and differences in sign designs across regions. Traditional methods relying on handcrafted features and classical machine learning struggle to handle these complexities effectively.\n",
            "\n",
            "The content of the paragraph 30 is ：The problem lies in developing a robust, efficient, and real-time Traffic Sign Recognition (TSR) system that can accurately detect and classify traffic signs under diverse environmental conditions. Such a system must be capable of handling a wide variety of traffic sign categories while addressing issues like class imbalance and computational limitations for deployment in edge devices or autonomous vehicles. Furthermore, the system should generalize well to new regions with different traffic sign standards and maintain reliability in dynamic scenarios like moving vehicles or urban environments.\n",
            "\n",
            "The content of the paragraph 31 is ：The goal is to leverage deep learning techniques, particularly Convolutional Neural Networks (CNNs) and advanced architectures like YOLO, Faster R-CNN, and Vision Transformers, to create a TSR system that achieves high accuracy, robustness, and real-time performance, ensuring safer and smarter transportation systems.\n",
            "\n",
            "The content of the paragraph 32 is ：\n",
            "\n",
            "The content of the paragraph 33 is ：IV. METHODOLOGY\n",
            "\n",
            "The content of the paragraph 34 is ：The methodology for Traffic Sign Recognition (TSR) using deep learning encompasses a series of critical steps that ensure the system can accurately detect and classify traffic signs under varying real-world conditions. The goal is to create a robust and efficient system that can handle challenges such as different lighting conditions, environmental variations, and occlusions.\n",
            "\n",
            "The content of the paragraph 35 is ：\n",
            "\n",
            "The content of the paragraph 36 is ：1. Data Collection and Preprocessing\n",
            "\n",
            "The content of the paragraph 37 is ：Traffic sign recognition begins with gathering relevant datasets such as the German Traffic Sign Recognition Benchmark (GTSRB), Belgian Traffic Sign Dataset (BTSD), or LISA Traffic Sign Dataset. These datasets are annotated with bounding boxes for detection and labels for classification. Data preprocessing involves resizing images to a uniform size, normalizing pixel values, and applying augmentation techniques like rotations, scaling, and flipping. This increases the diversity of training data and improves model robustness under real-world conditions.\n",
            "\n",
            "The content of the paragraph 38 is ：\n",
            "\n",
            "The content of the paragraph 39 is ：\n",
            "\n",
            "The content of the paragraph 40 is ：\n",
            "\n",
            "The content of the paragraph 41 is ：2. Model Design\n",
            "\n",
            "The content of the paragraph 42 is ：Use deep learning architectures such as Faster R-CNN, YOLO, or SSD for detection, and CNNs like ResNet or MobileNet for classification. Incorporate attention mechanisms for improved focus and lightweight models for edge device deployment.\n",
            "\n",
            "The content of the paragraph 43 is ：\n",
            "\n",
            "The content of the paragraph 44 is ：3. Training the Model\n",
            "\n",
            "The content of the paragraph 45 is ：Training involves splitting the dataset into training, validation, and testing sets. Loss functions such as cross-entropy for classification and multi-task loss for detection are used. Optimizers like Adam or SGD adjust model weights during training. Regularization techniques such as dropout and early stopping are employed to prevent overfitting, while hyperparameter tuning helps optimize model performance.\n",
            "\n",
            "The content of the paragraph 46 is ：\n",
            "\n",
            "The content of the paragraph 47 is ：4. Model Evaluation\n",
            "\n",
            "The content of the paragraph 48 is ：The performance of the trained model is assessed using key metrics to ensure it meets desired accuracy levels. For detection tasks, Intersection over Union (IoU), precision, recall, and mean Average Precision (mAP) are calculated to measure how well the model locates and identifies traffic signs. For classification tasks, metrics such as accuracy, precision, recall, and F1-score are used to evaluate how well the model assigns the correct label to each sign. Testing on a diverse, unseen dataset is crucial to verify the model's ability to generalize and avoid overfitting.\n",
            "\n",
            "The content of the paragraph 49 is ：\n",
            "\n",
            "The content of the paragraph 50 is ：5. Deployment\n",
            "\n",
            "The content of the paragraph 51 is ：Once trained, the model is optimized for deployment in real-time systems such as autonomous vehicles or Advanced Driver Assistance Systems (ADAS). Techniques like pruning, quantization, or knowledge distillation are applied to reduce the model's size and computational demands, ensuring it can run efficiently on edge devices with limited resources. Tools like TensorRT, OpenVINO, or custom hardware accelerators are used to accelerate inference and ensure the model can process traffic signs in real-time, crucial for safety in autonomous driving systems.\n",
            "\n",
            "The content of the paragraph 52 is ：\n",
            "\n",
            "The content of the paragraph 53 is ：6.DeploymentMonitoringand Updates\n",
            "\n",
            "The content of the paragraph 54 is ：After deployment, continuous monitoring of the model’s performance in real-world conditions is essential to identify any potential issues, such as misclassifications or failures in challenging environments. Data collected from real-time use can be leveraged to fine-tune the model and retrain it periodically to improve its accuracy and adaptability. This ongoing learning helps the system stay up-to-date with new traffic sign designs, environmental changes, or regional variations in traffic regulations, enhancing the robustness and long-term reliability of the recognition system.\n",
            "\n",
            "The content of the paragraph 55 is ：\n",
            "\n",
            "The content of the paragraph 56 is ：V. ARCHITECTURE\n",
            "\n",
            "The content of the paragraph 57 is ：\n",
            "\n",
            "The content of the paragraph 58 is ：\n",
            "\n",
            "The content of the paragraph 59 is ：\n",
            "\n",
            "The content of the paragraph 60 is ：\n",
            "\n",
            "The content of the paragraph 61 is ：ER Diagram \n",
            "\n",
            "The content of the paragraph 62 is ：\n",
            "\n",
            "The content of the paragraph 63 is ：\n",
            "\n",
            "The content of the paragraph 64 is ：VI.EXECUTION\n",
            "\n",
            "The content of the paragraph 65 is ：\n",
            "\n",
            "The content of the paragraph 66 is ：Fig1.1 Image detected\n",
            "\n",
            "The content of the paragraph 67 is ：\n",
            "\n",
            "The content of the paragraph 68 is ：VII.CONCLUSION\n",
            "\n",
            "The content of the paragraph 69 is ：Traffic Sign Recognition (TSR) using deep learning has become a vital technology for autonomous driving and advanced driver-assistance systems (ADAS). By using deep learning models like convolutional neural networks (CNNs), TSR systems can accurately detect and classify traffic signs, enhancing road safety and navigation.\n",
            "\n",
            "The content of the paragraph 70 is ：While significant progress has been made, challenges remain, including handling regional sign variations, ensuring robustness in diverse conditions, and optimizing models for real-time deployment on resource-constrained devices. Future research should focus on improving model generalization, enabling continuous learning, and increasing system efficiency.\n",
            "\n",
            "The content of the paragraph 71 is ：With ongoing advancements, TSR systems will become more reliable and integral to the development of autonomous vehicles, contributing to safer and more efficient transportation.\n",
            "\n",
            "The content of the paragraph 72 is ：\n",
            "\n",
            "The content of the paragraph 73 is ：VIII. FUTURE WORK\n",
            "\n",
            "The content of the paragraph 74 is ：Handling Variability in Traffic Signs\n",
            "Traffic signs vary in shape, color, and design across different countries and regions. Future research could focus on developing models that are adaptable to these variations, improving generalization to diverse sign designs. Techniques like domain adaptation could help fine-tune models for specific regions or sign types.\n",
            "\n",
            "The content of the paragraph 75 is ：\n",
            "\n",
            "The content of the paragraph 76 is ：Improving Robustness in Adverse Conditions\n",
            "TSR systems must perform reliably under various environmental conditions such as weather (rain, fog, snow), lighting (dawn, dusk, night), or partial occlusion of signs. Future work could explore advanced data augmentation, multi-spectral image analysis (using both RGB and infrared images), and models that improve robustness in challenging conditions.\n",
            "\n",
            "The content of the paragraph 77 is ：\n",
            "\n",
            "The content of the paragraph 78 is ：Real-Time and Low-Resource Deployment\n",
            "Deep learning-based TSR models often require substantial computational resources. Future work could focus on optimizing models for real-time deployment on edge devices by using model compression techniques like pruning, quantization, and knowledge distillation, enabling efficient performance on resource-constrained platforms.\n",
            "\n",
            "The content of the paragraph 79 is ：\n",
            "\n",
            "The content of the paragraph 80 is ：Continuous Learning and Model Adaptation\n",
            "TSR systems may encounter new or evolving traffic signs and changing environmental conditions. Implementing continuous learning techniques, such as online learning or active learning, would allow models to adapt over time, improving performance without needing full retraining.\n",
            "\n",
            "The content of the paragraph 81 is ：\n",
            "\n",
            "The content of the paragraph 82 is ：Explainability and Transparency\n",
            "As deep learning models become more complex, enhancing transparency becomes essential. Future research could focus on explainable AI (XAI) methods for TSR, helping developers and end-users understand model decisions, fostering trust, and ensuring safety in real-world deployments.\n",
            "\n",
            "The content of the paragraph 83 is ：\n",
            "\n",
            "The content of the paragraph 84 is ：IX. REFERENCES\n",
            "\n",
            "The content of the paragraph 85 is ：1.Dollár, P., Appel, R., Belongie, S., & Perona, P. (2012). Fast Feature Pyramids for Object Detection. IEEE Transactions on Pattern Analysis and Machine Intelligence, 36(8), 1532-1545.\n",
            "This paper discusses techniques for improving object detection, including methods that are applicable for detecting traffic signs in images.\n",
            "\n",
            "The content of the paragraph 86 is ：2.Yolo, J. (2016). You Only Look Once: Unified, Real-Time Object Detection. arXiv:1506.02640.\n",
            "The original paper on the YOLO (You Only Look Once) object detection framework, which is commonly used for detecting traffic signs in real-time systems.\n",
            "\n",
            "The content of the paragraph 87 is ：3.Berg, D., & Puch, M. (2018). Traffic Sign Recognition with Deep Convolutional Networks. In Proceedings of the International Conference on Computer Vision (ICCV).\n",
            "This paper investigates deep learning techniques for recognizing traffic signs and discusses the use of convolutional neural networks (CNNs) for classification.\n",
            "\n",
            "The content of the paragraph 88 is ：4.Grigorescu, S., Radoslav, L., & Visan, A. (2018). Deep Learning for Traffic Sign Recognition. Machine Vision and Applications, 29(1), 79-94.\n",
            "This work provides an in-depth look at applying deep learning techniques specifically for traffic sign recognition, exploring various CNN architectures and evaluation methods.\n",
            "\n",
            "The content of the paragraph 89 is ：5.Awais, M., & Asghar, S. (2020). Traffic Sign Recognition Using Convolutional Neural Networks and Region-based CNN. In Proceedings of the 5th International Conference on Artificial Intelligence and Computer Vision (AICV).\n",
            "This paper discusses the implementation of region-based CNNs, such as Faster R-CNN, for traffic sign detection and classification.\n",
            "\n",
            "The content of the paragraph 90 is ：\n",
            "\n",
            "The content of the paragraph 91 is ：\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLEkNQdvcill",
        "outputId": "4a507565-fe62-4666-b7cd-b36885a0844f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request as urllib2\n",
        "\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "_agyCA9DduM1"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = urllib2.urlopen('https://en.wikipedia.org/wiki/Natural_language_processing')\n",
        "\n",
        "html_doc = response.read()"
      ],
      "metadata": {
        "id": "gOQhs6G9dkJY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "# Formating the parsed html file\n",
        "\n",
        "strhtm = soup.prettify()\n",
        "\n",
        "# Print few lines\n",
        "\n",
        "print (strhtm[:5000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVhsP519dpt4",
        "outputId": "3acf62c5-f618-4e01-bb94-361ac2d93672"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<!DOCTYPE html>\n",
            "<html class=\"client-nojs vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\" dir=\"ltr\" lang=\"en\">\n",
            " <head>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <title>\n",
            "   Natural language processing - Wikipedia\n",
            "  </title>\n",
            "  <script>\n",
            "   (function(){var className=\"client-js vector-feature-language-in-header-enabled vector-feature-language-in-main-page-header-disabled vector-feature-page-tools-pinned-disabled vector-feature-toc-pinned-clientpref-1 vector-feature-main-menu-pinned-disabled vector-feature-limited-width-clientpref-1 vector-feature-limited-width-content-enabled vector-feature-custom-font-size-clientpref-1 vector-feature-appearance-pinned-clientpref-1 vector-feature-night-mode-enabled skin-theme-clientpref-day vector-sticky-header-enabled vector-toc-available\";var cookie=document.cookie.match(/(?:^|; )enwikimwclientpreferences=([^;]+)/);if(cookie){cookie[1].split('%2C').forEach(function(pref){className=className.replace(new RegExp('(^| )'+pref.replace(/-clientpref-\\w+$|[^\\w-]+/g,'')+'-clientpref-\\\\w+( |$)'),'$1'+pref+'$2');});}document.documentElement.className=className;}());RLCONF={\"wgBreakFrames\":false,\"wgSeparatorTransformTable\":[\"\",\"\"],\"wgDigitTransformTable\":[\"\",\"\"],\"wgDefaultDateFormat\":\"dmy\",\n",
            "\"wgMonthNames\":[\"\",\"January\",\"February\",\"March\",\"April\",\"May\",\"June\",\"July\",\"August\",\"September\",\"October\",\"November\",\"December\"],\"wgRequestId\":\"f0759aec-29af-423c-9c30-70125f545ddb\",\"wgCanonicalNamespace\":\"\",\"wgCanonicalSpecialPageName\":false,\"wgNamespaceNumber\":0,\"wgPageName\":\"Natural_language_processing\",\"wgTitle\":\"Natural language processing\",\"wgCurRevisionId\":1274942014,\"wgRevisionId\":1274942014,\"wgArticleId\":21652,\"wgIsArticle\":true,\"wgIsRedirect\":false,\"wgAction\":\"view\",\"wgUserName\":null,\"wgUserGroups\":[\"*\"],\"wgCategories\":[\"All accuracy disputes\",\"Accuracy disputes from December 2013\",\"Harv and Sfn no-target errors\",\"CS1 errors: periodical ignored\",\"CS1 maint: location\",\"Articles with short description\",\"Short description is different from Wikidata\",\"Articles needing additional references from May 2024\",\"All articles needing additional references\",\"All articles with unsourced statements\",\"Articles with unsourced statements from May 2024\",\"Commons category link from Wikidata\",\n",
            "\"Natural language processing\",\"Computational fields of study\",\"Computational linguistics\",\"Speech recognition\"],\"wgPageViewLanguage\":\"en\",\"wgPageContentLanguage\":\"en\",\"wgPageContentModel\":\"wikitext\",\"wgRelevantPageName\":\"Natural_language_processing\",\"wgRelevantArticleId\":21652,\"wgIsProbablyEditable\":true,\"wgRelevantPageIsProbablyEditable\":true,\"wgRestrictionEdit\":[],\"wgRestrictionMove\":[],\"wgNoticeProject\":\"wikipedia\",\"wgCiteReferencePreviewsActive\":false,\"wgFlaggedRevsParams\":{\"tags\":{\"status\":{\"levels\":1}}},\"wgMediaViewerOnClick\":true,\"wgMediaViewerEnabledByDefault\":true,\"wgPopupsFlags\":0,\"wgVisualEditor\":{\"pageLanguageCode\":\"en\",\"pageLanguageDir\":\"ltr\",\"pageVariantFallbacks\":\"en\"},\"wgMFDisplayWikibaseDescriptions\":{\"search\":true,\"watchlist\":true,\"tagline\":false,\"nearby\":true},\"wgWMESchemaEditAttemptStepOversample\":false,\"wgWMEPageLength\":60000,\"wgEditSubmitButtonLabelPublish\":true,\"wgULSPosition\":\"interlanguage\",\"wgULSisCompactLinksEnabled\":false,\"wgVector2022LanguageInHeader\":true,\n",
            "\"wgULSisLanguageSelectorEmpty\":false,\"wgWikibaseItemId\":\"Q30642\",\"wgCheckUserClientHintsHeadersJsApi\":[\"brands\",\"architecture\",\"bitness\",\"fullVersionList\",\"mobile\",\"model\",\"platform\",\"platformVersion\"],\"GEHomepageSuggestedEditsEnableTopics\":true,\"wgGETopicsMatchModeEnabled\":false,\"wgGEStructuredTaskRejectionReasonTextInputEnabled\":false,\"wgGELevelingUpEnabledForUser\":false};RLSTATE={\"ext.globalCssJs.user.styles\":\"ready\",\"site.styles\":\"ready\",\"user.styles\":\"ready\",\"ext.globalCssJs.user\":\"ready\",\"user\":\"ready\",\"user.options\":\"loading\",\"ext.cite.styles\":\"ready\",\"ext.math.styles\":\"ready\",\"skins.vector.search.codex.styles\":\"ready\",\"skins.vector.styles\":\"ready\",\"skins.vector.icons\":\"ready\",\"jquery.makeCollapsible.styles\":\"ready\",\"ext.wikimediamessages.styles\":\"ready\",\"ext.visualEditor.desktopArticleTarget.noscript\":\"ready\",\"ext.uls.interlanguage\":\"ready\",\"wikibase.client.init\":\"ready\",\"ext.wikimediaBadges\":\"ready\"};RLPAGEMODULES=[\"ext.cite.ux-enhancements\",\"ext.scribunto.logs\",\"site\",\n",
            "\"mediawiki.page.ready\",\"jquery.makeCollapsible\",\"mediawiki.toc\",\"skins.vector.js\",\"ext.centralNotice.geoIP\",\"ext.centralNotice.startUp\",\"ext.gadget.ReferenceTooltips\",\"ext.gadget.switcher\",\"ext.urlShortener.toolbar\",\"ext.centralauth.centralautologin\",\"mmv.bootstrap\",\"ext.popups\",\"ext.visualEditor.desktopArticleTarget.init\",\"ext.visu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FtSbLaXbdy1P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}